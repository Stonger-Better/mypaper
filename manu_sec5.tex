%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
\usepackage{amssymb,amsmath}
\usepackage{array,tabularx,multirow}
\usepackage[linesnumbered,boxed]{algorithm2e}
%\usepackage{algorithm}
%\usepackage{algorithmic}
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}

%\newtheorem{lemma}{\bf{Lemma}}[section]

%\newtheorem{theorem}{\bf{Theorem}}[section]

%\newtheorem{definition}{Definition}[section]
% Insert the name of "your journal" with
% \journalname{myjournal}

\begin{document}

\title{A Preconditioning Algorithm for Large Linear Systems Based on A Low-stretch Spanning Tree% Based on a Low-stretch Spanning Tree  %\thanks{Grants or other notes
%about the article that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}
%A practical preconditioning algorithm for Laplacian(SDD) linear system
}
%\subtitle{Do you have a subtitle?\\ If so, write it here}

%\titlerunning{Short form of title}        % if too long for running head

\author{Huirong Zhang \and  Jianwen Cao \and Xiaohui Liu  %etc.
}%\inst{1,2}

%\authorrunning{Short form of author list} % if too long for running head

\institute{H.R. Zhang \at
              State Key Laboratory of Computer Science, Institute of Software Chinese Academy of Sciences. \\
              \email{zhang06.happy@163.com}           %  \\
%%             \emph{Present address:} of F. Author  %  if needed
           \and
           J.W. Cao \at
             State Key Laboratory of Computer Science, Institute of Software Chinese Academy of Sciences.\\
           \and
           X.H. Liu  \at
              State Key Laboratory of Computer Science, Institute of Software Chinese Academy of Sciences.\\
}

%\institute{  State Key Laboratory of Computer Science, Institute of Software Chinese Academy of Sciences. \\
%              \email{zhang06.happy@163.com}           %  \\
%%             \emph{Present address:} of F. Author  %  if needed
%           \and
%             State Key Laboratory of Computer Science, Institute of Software Chinese Academy of Sciences.\\
%          }


\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle

\begin{abstract}
In this paper, we present an efficient algorithm for preconditioning sparse, symmetric, diagonally-dominant(SDD) linear systems by using combinatorial preconditioning techniques.  Firstly, we build a low-stretch spanning tree of a graph associated with a linear system by using algorithm proposed by Alon et al. and  add appropriate high stretch edges to the tree straightly or add edges based on a tree-decomposition algorithm to get an optimized subgraph. Then, convert the subgraph into a SDD matrix and take it as a preconditioner. %solve a  SDD system $Ax=b$ preconditioned by a matrix converted from the subgraph we create.
Finally, we give an implementation and  performance analysis of our subgraph preconditioners. %We show experimentally that these combinatorial preconditioners  have robust convergence and  have good scalability.
We test the algorithm on extensive numerical experiments arising from both elliptic PDEs and  Laplacian systems of network graphs. Numerical experiments show that preconditioners constructed by our algorithm are more efficient than incomplete Choleskey factorization preconditioners and Vaidya's preconditioners. Moreover, our preconditioning algorithm is insensitive to the boundary condition and it scales well. %, which means our combinatorial preconditioning algorithm has good scalability.
%In addition, for the 2-D problems considered in this paper, preconditioners obtained based on a low stretch tree have better performance than  incomplete
%Cholesky preconditioners, which is sensitive to boundary  and anisotropy.
  Besides, the efficiency of our subgraphs preconditioners depends on not only the stretch  but also depends on the sparsity.  %But they are % and matrix size.


%\noindent {\bf AMS subject classifications: }
\keywords{preconditioning algorithm \and linear systems  \and low-stretch spanning tree \and tree-decomposition \and experimental analysis}
%\keywords{PCG Algorithm;  Augmented low stretch spanning tree; Tree-decomposition; SDD linear systems}
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}





\section{Numerical Experiments}\label{sec:4}

This section presents some numerical experiments. The experiments are conducted on
%both a dual-processor computer (running on windows)
% 3.10 GHz Intel Core i5 with 3 GB of main memory and
a computer server (32GHz 64-bit Intel Xeon, running Ubuntu, connected via Ethernet). %distributed-memory cluster%We only use one processor.  Numerical results show that our scheme performs well on both. In addition,
%We implement the Algorithm \ref{akpw_all} in C and all the experiments are tested by using a package called TAUCS implemented by Toledo etc. \cite{taucs}.
%In this paper, we report the results of numerical experiments only on a distributed-memory cluster.

%The tested SDD matrices of linear systems arising from both mesh like graphs and unstructured network graphs.
%The right hand sides are generated randomly.
%arise from Poisson problems(considering both isotropic and anisotropic cases) and arise from unweighted graphs(including both mesh like graphs and unstructured network graphs)
The tested SDD matrices of linear systems consist of two parts. The first part of matrices used for our experimental analysis arise from  finite differences discretization of the  elliptic PDEs  problems\cite{Chen_Tov,HCS09}. Another part of tested matrices(shown in Table \ref{tab_net}) are Laplacian matrices in unweighted graphs from the UF Sparse Matrix Collection \cite{UF}.
  All of the  matrices are diagonally-dominant  matrices with non-positive off diagonals.
The singularity of singular systems (Poisson problem with Neumann boundary conditions or Laplacian matrices of unweighted network graphs) are dealt with by forcing the first diagonal entry of the coefficient matrix to be added by 1. Concretely, if $A$  is the coefficient matrix of a singular system, take $A(1,1)=A(1,1)+1$. The right hand sides are generated randomly.

We test the performance of our subgraph preconditioning algorithm (denote the preconditioners as ALST), and compare to other  preconditioners such as incomplete Cholesky  factorization (denote as IC)  and  Vaidya's preconditioners (denote as Vaidya) applied with a conjugate gradients iterative solver.
In our  experiments, the initial guess is a vector generated randomly and the iteration terminates once  the  $2$-norm of residual reduces  by a factor of $10^{8}$.



These experiment results all show that our preconditioning algorithm is more efficient than drop-tolerance incomplete Cholesky factorization(IC) preconditioners and Vaidya's preconditioners.
%The numerical experiments show that our preconditioning algorithm is more efficient than drop-tolerance incomplete Cholesky factorization(IC) preconditioners and Vaidya's preconditioners not only for linear systems arising from Poisson problems(considering  both isotropic and anisotropic cases), but also for  linear systems arising from unweighted graphs(including both mesh like graphs and unstructured network graphs). %The right hand sides are generated randomly.
%combinatorial preconditioners obtained by our algorithm  have robust convergence.
%They are insensitive to the numerics of the problem. Their performance does not vary much when we change the boundary conditions of the problems, or when we change the direction of anisotropy in anisotropic problems.
 The first  part of test examples  show that our combinatorial preconditioning algorithm (Algorithm \ref{com_pre}) scales well, as the number of iterations grow slowly as mesh size grows. %, which means our combinatorial preconditioning algorithm has good scalability.
%In addition, for the 2-D problems considered in this paper, preconditioners obtained based on a low stretch tree have better performance than  incomplete
%Cholesky preconditioners, which is sensitive to boundary  and anisotropy.
 Moreover, the smallest generalized eigenvalues of a linear system preconditioned by a subgraph preconditioner is 1, just as analyzed in theory.
The second part of examples show that  when a spanning tree of the underlying graph of a matrix has  low  average stretch, %or small max stretch at the same time,
a preconditioner constructed based on such a spanning tree applied with a CG solver converges fast. Besides, the efficiency of our subgraphs preconditioners depends on not only the
stretch  but also depends on the sparsity.
%condition number of matrices but also depends on the sparsity and matrix size.

%approximate the matrix well in terms of spectral distribution, which means











%This section presents some numerical experiments. The experiments are conducted on
%both a dual-processor computer (running on windows)
% 3.10 GHz Intel Core i5 with 3 GB of main memory and a distributed-memory cluster(2.00GHz 64-bit Intel Xeon, running Ubuntu, connected via Ethernet). %We only use one processor. 这句话干脆不要了
%   Numerical results show that our scheme performs well on both. In addition,
%we implement the Algorithm \ref{akpw_all} in C and all the experiments are tested by using a package called TAUCS implemented by Toledo et al. \cite{taucs}.
%In this paper, we report the results of numerical experiments only on a distributed-memory cluster.

%The tested SDD matrices of linear systems arising from both mesh like graphs and unstructured network graphs.
%The right hand sides are generated randomly.
%The tested SDD matrices of linear systems arising from Poisson problems (considering  both isotropic and anisotropic cases) and arising from unweighted graphs (including both mesh like graphs and unstructured network graphs). We test our subgraph preconditioning algorithm (denote the preconditioners as AKPW), and compare to other  preconditioners such as incomplete Cholesky  factorization (denote as IC)  and  Vaidya's preconditioners (denote as Vaidya) applied with a conjugate gradients iterative solver.
%In our experiments, the initial guess is a vector generated randomly, and the right hand sides are generated randomly too. The iteration will terminate once the $2$-norm of residual reduces  by a factor of $10^{8}$.


%We are unable to find large unstructured PSDDD matrices  in matrix collections, such as Matrix Market and Tim Davis’ s collection.
%are generated from the software of TAUCS, which are  a class of matrices that
\subsection{Elliptic PDEs  problem}
 The first part of matrices used for our experimental analysis arise from  finite differences discretization of the  following elliptic PDEs  problems \cite{Chen_Tov,HCS09}.
 %, whose underlying graphs are mesh like graphs.

%partial differential equation  on  regular 2-dimensional  meshes.

%Matrices  arise from the following elliptic PDEs  problem.

\begin{equation}\label{eq}
  c_x\frac{\partial^2 u}{\partial x^2}+c_y\frac{\partial^2 u}{\partial y^2}=f \quad in~~ (0,1)\times (0,1)
\end{equation}
We consider isotropic problems ($c_x=c_y=1$) with either Dirichlet or Neumann boundary conditions and anisotropic problems in which either $c_x=100$ and $c_y=1$ or vice versa.
%with either Dirichlet or Neumann boundary conditions. We consider isotropic problems($c_x=c_y=1)

%For convenience, we give the following symbol descriptions,
%\begin{description}
%  \item[\bf{$A\_iso\_D\_X$:}] A matrix in size $X$ arising from  isotropic problems($c_x=c_y=1$) with Dirichlet boundary conditions.
%  \item[\bf{$A\_iso\_N\_X$:}] A matrix in size $X$ arising from  isotropic problems($c_x=c_y=1$) with Neumann boundary conditions.
%  \item[\bf{$A\_aniso\_x\_X$:}] A matrix in size $X$ arising from anisotropic problems($c_x=100$ and $c_y=1$) with Neumann boundary conditions.
% \item[\bf{$A\_aniso\_y\_X$:}] A matrix in size $X$ arising from anisotropic problems($c_x=1$ and $c_y=100$) with Neumann boundary conditions.
%\end{description}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 此处分为两部分，所以这里要隔开
%Another part of tested matrices(shown in Table \ref{tab_net}) are Laplacian matrices of unweighted graphs from the UF Sparse Matrix Collection \cite{UF}. %which are Laplacian matrices  of a weighted, undirected graph.

%We perform
%Another part of tested matrices are arise from a variety of unweighted graphs from the UF Sparse Matrix Collection \cite{UF}, which are Laplacian matrices  of a weighted, undirected graph.

%We consider isotropic problem($c_x=c_y=1$) with Dirichlet or Neumann boundary conditions and anisotropic problems with either $c_x=100$ and $c_y=1$ or vise versa of Neumann boundary %conditions. Denote $A\_X\_iso_D$ by a matrix in size $X$ arising from  Dirichlet boundary conditions  problems.


%%All of the resulting matrices are diagonally-dominant  matrices with non-positive off diagonals.
%The singularity of singular systems (Poisson problem with Neumann boundary conditions or Laplacian matrices of unweighted network graphs) are dealt with by forcing the first diagonal entry of the coefficient matrix to be added by 1. Concretely, if $A$  is the coefficient matrix of a singular system, take $A(1,1)=A(1,1)+1$.
% If matrices arise from

%另一部分矩阵来源于UFmatrix collection. 其中矩阵都是无向图Laplace矩阵，并做了一些处理，就是上面的去除奇异化.  其条件数都很大。






%\begin{table}[H]\fontsize{8.5pt}{12pt}\selectfont
%  \begin{center}
%  \caption{}\vspace{1pt}
%    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}*{9}{|c|c| c| c| c|c| c| c| c}} \cline{1-9}
%    \multirow{2}{*}{Matrix} & \multicolumn{4}{c|}{IC} & \multicolumn{4}{c|}{LST}\\
%    \cline{2-9}
%
%        & $t\_f$  & $t\_s$ & $it$ & $order$& $t\_f$  & $t\_s$ & $it$ & $order$\\   \cline{1-9}
%       data  & 2851  &33037 &11.5879& 0.0037 &4.3882e-006 &4.9612e+003 &binary&\\  \cline{1-9}
%
%
%  \end{tabular*}\label{tab_Poi}%\vspace{-15pt}
%  \end{center}
%\end{table}









%\begin{table}[ht]
%% table caption is above the table
%\caption{Tested matrices who  arise from 2D Poisson problem or are Laplacian matrices arising  from other problems with 2D geometry.}\label{mesh} %with  underlying graphs of structured mesh
%\begin{tabular}{cccccccc}
%\hline\noalign{\smallskip}
%mesh size& 300 & 400 &500&600&700&800&900  \\
%\noalign{\smallskip}\hline\noalign{\smallskip}
%iterations&98 & 110 & 115& 119 &127&126&138 \\
%%number & number & number \\
%\noalign{\smallskip}\hline
%\end{tabular}
%\end{table}
%
%
%
%
%\begin{table}[ht]
%% table caption is above the table
%\caption{Tested matrices who are Laplacian matrices of unweighted network graphs. Here $n$ is the number matrix rows, $m$ is the number of nonzeros in a matrix, and the sparsity $(m/n)$ is the average number of nonzeros in a column of a matrix. $\lambda_{min}$,  $\lambda_{max}$ are the minimal and maximal  eigenvalue of a matrix respectively, cond(A) denotes the condition number of matrix $A$}\label{network}
%\begin{tabular}{cccccccc}
%\hline\noalign{\smallskip}
%mesh size& 300 & 400 &500&600&700&800&900  \\
%\noalign{\smallskip}\hline\noalign{\smallskip}
%iterations&98 & 110 & 115& 119 &127&126&138 \\
%%number & number & number \\
%\noalign{\smallskip}\hline
%\end{tabular}
%\end{table}






%In the experiments well conditioned systems(isotropic problem) we reduce the  $2-$norm of residual  by a factor of $10^{8}$, while, experiments on  ill conditioned systems(aniso) reduce the residual by $10^{15}$.

% Firstly, we give the nonzero structure of matrices(Figure \ref{matshape})
% %we use and the structure of the preconditioners(Figure \ref{preshape}) we consider, including the preconditioners obtained by the Algorithm \ref{com_pre} using algorithm Sub$\_$Decomp in it (denote by LST ), Vaidya's preconditioners(augmented maximum spanning tree)(denote by MST),  drop-tolerance incomplete Cholesky (denote by IC ) and modified incomplete Cholesky preconditioners(with relaxed diagonal modification)(denote by MIC).

 %Then we observe  the spectral distribution of preconditioned system with different preconditioners. As Figure \ref{iso_anisoeig} shown, preconditioned system with MIC and IC preconditioners have a number of very small eigenvalues and a number of larger eigenvalues. While most of the eigenvalues of the preconditioned system with  MST and LST  preconditioners are almost 1, which is just as predicted by the theoretical analysis. Moreover,  Figure \ref{iso_anisoeig} shows that preconditioners obtained by Algorithm \ref{com_pre} using a tree decomposition have more clustered eigenvalues distribution than a low stretch tree and than Vaidya's preconditioners which use the same tree decomposition algorithm for augmenting a maximum weight spanning tree. All of the preconditioners have almost the same nonzeros in $L$(preconditioners' incomplete Cholesky factorization).

%\begin{figure}[ht]
%% Use the relevant command to insert your figure file.
%% For example, with the graphicx package use
% \includegraphics[height=7cm,width=0.9\textwidth]{mat.eps}
%% figure caption is below the figure
%\caption{The nonzero structure of coefficient matrices of the linear system for anisotropic problem with x-direction of strong influences and for isotropic problem with Dirichlet boundary condition. All of the preconditioners have almost the same nonzeros in $L$(preconditioners Cholesky factorization).}
%\label{matshape}       % Give a unique label
%\end{figure}
%
%
%\begin{figure}[ht]
%% Use the relevant command to insert your figure file.
%% For example, with the graphicx package use
% \includegraphics[height=9.cm,width=1.0\textwidth]{pre.eps}
%% figure caption is below the figure
%\caption{The nonzero structure of preconditioners for anisotropic problem with x-direction of strong influences.}
%\label{preshape}       % Give a unique label
%\end{figure}
%





%
%As the MIC preconditioners don't always outperform IC preconditioners and the combinatorial preconditioners obtained by Algorithm \ref{com_pre} we give always outperform  MIC and IC. In next experiments, we just list the result about the performance of our combinatorial preconditioners and the  comparison with IC  and Vaidya's preconditioners(augmented maximum spanning tree)


Now we show the experiment results for matrices arising from isotropic problems and anisotropic problems respectively.
%\subsection{isotropic problems}
% We firstly observe the performance of Algorithm \ref{sub_add} and Algorithm \ref{sub_decom}  applied in Algorithm \ref{com_pre} by testing the generalized eigenvalues distribution on a small model.
 We firstly observe the performance  of Algorithm \ref{sub_add} and Algorithm \ref{sub_decom}  applied in Algorithm \ref{com_pre} by testing the generalized eigenvalues distribution on a small model. In Fig \ref{isoeig}, AKPW denotes the low stretch spanning tree preconditioner, AKPW$\_$add denotes the augmented low stretch tree preconditioner obtained by  Algorithm \ref{sub_add} applied in Algorithm \ref{com_pre},  and  AKPW$\_$treedecomp denotes the augmented low stretch tree preconditioner obtained by  Algorithm \ref{sub_decom} applied in Algorithm \ref{com_pre}.
\begin{figure}[htp]
% Use the relevant command to insert your figure file.
% For example, with the graphicx package use
 \includegraphics[height=6.3cm,width=0.82\textwidth]{AKPW_add_decom.eps}%height=5cm,
% figure caption is below the figure
\caption{The generalized eigenvalues of isotropic problems and anisotropic problems  preconditioned by Laplacian of a low stretch tree and Laplacians of subgraphs obtained by using Algorithm \ref{sub_add} and Algorithm \ref{sub_decom} in Algorithm \ref{com_pre} respectively.} % with nearly the same number of nonzeros in the latter two subgraphs preconditioners }
\label{isoeig}       % Give a unique label
\end{figure}


Figure \ref{isoeig} shows that both the Algorithm \ref{sub_add} and Algorithm \ref{sub_decom} improve the low stretch spanning tree and the smallest generalized eigenvalues of a linear system preconditioned by a subgraph preconditioner is 1, just as analyzed in theory. But the scheme of adding edges by tree decomposition has a better improvement on the low-stretch tree than the scheme of adding edges with high stretch edges straightly.

%For Dirichlet boundary condition case, CG with a preconditioner obtained by using algorithm Sub$\_$Add in Precon$\_$Create takes 80 iterations while with a preconditioner obtained by using algorithm Sub$\_$Decomp in Precon$\_$Create takes 53 iterations when both add only 382 edges to the tree.
%For Neumann boundary condition case, CG with a preconditioner obtained by using algorithm Sub$\_$Add in Precon$\_$Create takes 85 iterations while with a preconditioner obtained by using algorithm Sub$\_$Decomp in Precon$\_$Create takes 54 iterations when both add only 382 edges to the tree.

%For larger size model, such an advantage of algorithm Sub$\_$Decomp over  Sub$\_$Add is highlighted,
In fact, adding edges straightly is easy to produce fill-in when factors the preconditoner, but, adding edges by using tree decomposition can preserve sparsity in some degree.

In the following examples we just test our combinatorial preconditioners obtained by using Algorithm \ref{sub_decom} in the Algorithm \ref{com_pre} and denote the corresponding preconditioners as ALST.

%we don't list the comparison diagram here.
%Besides, adding edges straightly is easy to produce fill-in when factors  the preconditoner, but, adding edges by using tree decomposition can preserve sparsity in some degree.


Next observe the convergence of PCG preconditioned by three kinds of preconditioners. Figure \ref{isoiter} shows the number of iterations of PCG takes. All the preconditioners have nearly $4n$ nonzeros in $L$ (the factors of the preconditioners).   %our subgraphs preconditioners based on low stretch tree (LST), and compare to the preconditioners of vaidya's augmented maximum weight spanning tree (MST), and incomplete Cholesky factorization (IC).
% Figure \ref{isoiter} shows the number of iterations of PCG takes when the $2-$norm of residuals reduce by a factor of $10^{8}$. All the preconditioners have nearly $4n$ nonzeros in $L$ (the factor of the preconditioners).

As shown in Figure \ref{isoiter}, ALST preconditioners converge much faster than IC preconditioners. Moreover, our preconditioners do not vary much when we change the boundary conditions.
\begin{figure}[htp]
% Use the relevant command to insert your figure file.
% For example, with the graphicx package use
 \includegraphics[width=0.85\textwidth]{akpw_iter_size.eps}
% figure caption is below the figure
\caption{The convergence of Vaidya(augmented maximum weight spanning tree preconditioner), ALST(our subgraph preconditioner based on low stretch spanning tree), IC(drop-tolerance incomplete Cholesky factorization) for isotropic problems and anisotropic problems. The figure shows iterations of PCG when converges with all the preconditioners having nearly $4n$ nonzeros in $L$. }
\label{isoiter}       % Give a unique label
\end{figure}

In order to compare the performance of Vaidya's preconditioners and ALST preconditioners, we plot a figure (Fig. \ref{lm_iter}) only contain these two kinds of preconditioner.
In fact, for isotropic problems, the underlying graphs of the coefficient matrices are unweighted. In this case, any spanning tree is a maximum weight spanning tree. Therefore, a low stretch spanning tree is a  more reasonable choice for preconditioning Laplacian systems with unweighted underlying graphs, especially for very large systems.

 As shown in Figure \ref{lm_iter}, ALST preconditioners that based on  low stretch spanning trees are more efficient than Vaidya's  preconditioners that based on maximum spanning trees for both weighted case (isotropic problems) and unweighted case (anisotropic problems). Just like theoretical analysis.

%As shown in Figure \ref{isoiter} and Figure \ref{lm_iter}, our preconditioners obtained by Algorithm Precon$\_$Create  scale well for isotropic
%problems with the mesh size, which is similar to Vaidya and even better than Viadya in our experiments. The number of iterations increase slowly as the mesh size increase.
%Moreover, our preconditioners do not vary much when we change the boundary conditions.
%这里说明一下其实对于isotropic情形，任意的图都是MST，所以，实验结果中LST优于MST，正如理论分析，MST并没有LST逼近效果更好。











\begin{figure}[htp]
% Use the relevant command to insert your figure file.
% For example, with the graphicx package use
 \includegraphics[width=0.85\textwidth]{akpw_lm_mesh.eps}%height=5cm
% figure caption is below the figure
\caption{The convergence of Vaidya(augmented maximum weight spanning tree preconditioner), ALST(our subgraph preconditioner based on low stretch spanning tree), for isotropic problems and anisotropic problems. The figure shows iterations of PCG when converge with all the preconditioners having nearly $4n$ nonzeros in $L$.}
\label{lm_iter}       % Give a unique label
\end{figure}

%For isotropic problems, the underlying graphs of the coefficient matrices are unweighted, in which case any tree is a maximum weight spanning tree. This just is the reason that LST preconditioners have better convergence than MST preconditioners. We give a more specific explanation by comparing the max stretch and average stretch of MST and LST. Take the isotropic problem with Dirichlet boundary condition of mesh size $100\times 100$ for example, Figure \ref{stretch_size} show the max stretch and average stretch  of the maximum spanning tree are larger than that of the low stretch spanning tree. Just as analyzed in Theorem \ref{basic}, a spanning tree with low  average stretch or small max stretch at the same time can be a good preconditioner that well approximates the spectral distribution of a Laplacian matrix. For isotropic problem with Neumann boundary condition, the max stretch and average stretch  of a low stretch spanning tree equal those of isotropic problem with Dirichlet boundary condition, which help explain why the our preconditioner have nearly the same preconditioning results for different boundary conditions. In fact, combinatorial preconditioners are insensitive to numeric values  but depend on the nonzero structure of matrices.
%
%\begin{figure}[ht]
%% Use the relevant command to insert your figure file.
%% For example, with the graphicx package use
% \includegraphics[height=8.cm,width=0.9\textwidth]{stretch_size.eps}
%% figure caption is below the figure
%\caption{The max stretch and average stretch for the isotropic problem with Dirichlet boundary condition of mesh size $100\times 100$.}
%\label{stretch_size}       % Give a unique label
%\end{figure}



%Although the more nonzeros in $L$ (the incomplete factorization of preconditioners)?????? , the less iterations PCG takes. The increasing factor time is more than solve time for CG iterations decreasing when there is too many zeros in preconditioners ???????. We need add appropriate edges to balance factor time and solve time. We define the fill-in ratio $$\gamma=nnz(L)/(2n),$$
%where $nnz(L)$ is the number of nonzeros in $L$, $n$ is the rows of matrix $A$. Take isotropic problems for example, Figure \ref{isofill} shows the time (solve time+factor time) of PCG cost with different fill-in ratios.



After adding edges to the tree, the condition number of the preconditioned system is improved, resulting in a better convergence rate. However, this is accompanied with an increase in the cost of factorization and the work required in each iteration step. To balance these, we need add appropriate number of edges . We define  fill-in ratio $\gamma=nnz(L)/(2n)$, where $nnz(L)$ is the number of nonzeros in $L$ (the incomplete factorization of preconditioners), $n$ is the rows of matrix $A$. Take isotropic problems for example, Figure \ref{isofill} shows the time (solve time+factor time) of PCG cost with different fill-in ratios.
\begin{figure}[htp]
% Use the relevant command to insert your figure file.
% For example, with the graphicx package use
 \includegraphics[width=0.85\textwidth]{akpw_fill.eps}%height=5cm
% figure caption is below the figure
\caption{The time cost of PCG when preconditioned by Vaidya(augmented maximum weight spanning tree preconditioner), ALST(our subgraph preconditioner based on low stretch spanning tree), IC(drop-tolerance incomplete Cholesky factorization) for isotropic problems with Neumann and Dirichlet boundary conditions  as fills in $L$ increase.}
\label{isofill}       % Give a unique label
\end{figure}

We test the iterations of PCG takes when set the fill-in ratio $\gamma\approx 2.5$.  Table \ref{isotab} and Table \ref{anisotab} show that iterations grow slowly as mesh size grows, which reflects the good scalability of ALST preconditioners.

\begin{table}[!htbp]
% table caption is above the table
\caption{The number of iterations for  preconditioners obtained by Algorithm \ref{com_pre} using Sub$\_$Decomp on isotropic problems with Neumann boundrary condition). There are nearly 5n nonzero entries in $L$, $n$ is the row number of matrices. The reduction in the residual is by a factor of $10^{8}$. }
\label{isotab}       % Give a unique label
% For LaTeX tables use
\begin{tabular}{cccccccc}
\hline\noalign{\smallskip}
mesh size& 300 & 400 &500&600&700&800&900  \\
\noalign{\smallskip}\hline\noalign{\smallskip}
iterations&98 & 110 & 115& 119 &127&126&138 \\
%number & number & number \\
\noalign{\smallskip}\hline
\end{tabular}
\end{table}



\begin{table}[!htbp]
% table caption is above the table
\caption{The number of iterations for  preconditioners obtained by Algorithm \ref{com_pre} using Sub$\_$Decomp on anisotropic problems(both for x and y directions). There are nearly $5n$ nonzero entries in $L$, $n$ is the row number of matrices. The reduction in the residual is by a factor of $10^{8}$. }
\label{anisotab}       % Give a unique label
% For LaTeX tables use
\begin{tabular}{cccccccc}
\hline\noalign{\smallskip}
mesh size& 300 & 400 &500&600&700&800&900  \\
\noalign{\smallskip}\hline\noalign{\smallskip}
iterations&47 & 51 & 56& 51 &53&53&54 \\
%number & number & number \\
\noalign{\smallskip}\hline
\end{tabular}
\end{table}

We also test the ALST preconditioners for 3-D elliptic problems. They are efficient than Vaidya's preconditioners but are not efficient than IC preconditioners for isotropic elliptic problems in 3-D. Concerning this issue, we develop an efficient algorithm for large Laplacian system in 3-D mesh. The result will be shown in another paper.
%The data in Figure \ref{isofill} demonstrates that fill-in ratio between the interval $[2,3]$ may be a good choice.

%We test the iterations of PCG takes when set the fill-in ratio $\gamma\approx 2.5$.  Table \ref{isotab} and Table \ref{anisotab} show that iterations grow slowly as mesh size grows, which embody the good scalability of AKPW preconditioners.% obtained by  algorithm Precon$\_$create(we just take list the result for the Neumann boundary condition case).
%As shown in Figure \ref{isoiter} and Figure \ref{lm_iter}, our preconditioners obtained by Algorithm Precon$\_$Create  scale well for isotropic
%problems with the mesh size, which is similar to Vaidya and even better than Viadya in our experiments.


%\begin{table}[ht]
%% table caption is above the table
%\caption{The number of iterations for  preconditioners obtained by algorithm Precon$\_$Create using Sub$\_$Decomp on isotropic problems with Neumann boundrary condition). There are nearly 5n nonzero entries in $L$, $n$ is the row number of matrices. The reduction in the residual is by a factor of $10^{8}$. }
%\label{isotab}       % Give a unique label
%% For LaTeX tables use
%\begin{tabular}{cccccccc}
%\hline\noalign{\smallskip}
%mesh size& 300 & 400 &500&600&700&800&900  \\
%\noalign{\smallskip}\hline\noalign{\smallskip}
%iterations&98 & 110 & 115& 119 &127&126&138 \\
%%number & number & number \\
%\noalign{\smallskip}\hline
%\end{tabular}
%\end{table}



%\begin{table}[ht]
%% table caption is above the table
%\caption{The number of iterations for  preconditioners obtained by algorithm Precon$\_$Create using Sub$\_$Decomp on anisotropic problems(both for x and y directions). There are nearly $5n$ nonzero entries in $L$, $n$ is the row number of matrices. The reduction in the residual is by a factor of $10^{8}$. }
%\label{anisotab}       % Give a unique label
%% For LaTeX tables use
%\begin{tabular}{cccccccc}
%\hline\noalign{\smallskip}
%mesh size& 300 & 400 &500&600&700&800&900  \\
%\noalign{\smallskip}\hline\noalign{\smallskip}
%iterations&47 & 51 & 56& 51 &53&53&54 \\
%%number & number & number \\
%\noalign{\smallskip}\hline
%\end{tabular}
%\end{table}

%\subsection{anisotropic problems}
% We next observe the performance  of algorithms Sub$\_$Add and Sub$\_$Decomp for anisotropic problems by testing the generalized eigenvalues distribution also on a small model.
%%\begin{figure}[ht]
%%% Use the relevant command to insert your figure file.
%%% For example, with the graphicx package use
%% \includegraphics[height=5cm,width=0.9\textwidth]{aniso_eig.eps}
%%% figure caption is below the figure
%%\caption{The generalized eigenvalues of anisotropic problems with Neumann  boundary conditions preconditioned by Laplacian of a low stretch tree(LST) and Laplacians of subgraphs obtained by algorithms Sub$\_$Add(LST$\_$add) and Sub$\_$Decomp(LST$\_$treedecomp) with nearly the same number of nonzeros in the latter two subgraphs preconditioners }
%%\label{anisoeig}       % Give a unique label
%%\end{figure}
%
%
%Figure \ref{isoeig} shows that both the algorithms Sub$\_$Add and Sub$\_$Decomp are also effective for anisotropic problems. Sub$\_$Decomp is more effective than Sub$\_$Add as the result obtained by using Sub$\_$Decomp in algorithm Precon$\_$Create has smaller max eigenvalue and more clustered spectral distribution than that of Sub$\_$Add. For Dirichlet boundary condition case, CG with a preconditioner obtained by using algorithm Sub$\_$Add in Precon$\_$Create takes 66 iterations while with a preconditioner obtained by using algorithm Sub$\_$Decomp in Precon$\_$Create takes 36 iterations when both add only 343 edges to the tree.For strong influences in x direction case, CG with a preconditioner obtained by using algorithm Sub$\_$Add in Precon$\_$Create takes 65 iterations while with a preconditioner obtained by using algorithm Sub$\_$Decomp in Precon$\_$Create takes 36 iterations when both add only 343 edges to the tree.
%
%On anisotropic problems, our preconditioners converge faster than on an isotropic problem.
%
%Next observe the convergence of PCG preconditioned by our subgraphs preconditioners based on low stretch tree (denote this preconditioner by LST),and compare to the preconditioners of vaidya's augmented maximum weight spanning tree (denote this preconditioner by MST), and incomplete Cholesky factorization (denote this preconditioner by IC) for anisotropic problem.
%Just like isotropic case, we need set a appropriate fill-in  ratio to balance the factor cost and iteration cost. For anisotropic problem, we set $\gamma\approx 1.74$.
%Figure \ref{anisoiter} shows the number of iterations of PCG takes when the $2-$norm of residuals reduce by a factor of $10^{15}$. All the preconditioners have nearly $3.5n$ nonzeros in $L$.
%
%In order to observe the performance of Vaidya's preconditioners  and  our combinatorial preconditioners, we plot a graph only contain these two kinds of preconditioner as Figure \ref{lm_iter}.
%As shown in Figure \ref{anisoiter} and Figure \ref{lm_iter}, our preconditioners obtained by Algorithm Precon$\_$Create  scale well for anisotropic
%problems with the mesh size. The number of iterations increase slowly as the mesh size increase.
%Moreover, for the case of anisotropic
%problem with strong influences in y direction, there is the same result, which means
% our preconditioners are insensitive to the anisotropy.
%This can be explained by that the max stretch and average stretch  of a low stretch spanning tree corresponding to problem with strong influences in x direction equal to those of case for problem with strong influences in y direction. We don't list the specific values about the stretch here.
%We test the iterations of PCG takes when $\gamma\approx 2.5$. Table \ref{anisotab} shows that iterations grows slowly as mesh size grows, which scales better than on isotropic problems.
%
%\begin{table}[ht]
%% table caption is above the table
%\caption{The number of iterations for  preconditioners obtained by algorithm Precon$\_$Create using Sub$\_$Decomp on anisotropic problems(both for x and y directions). There are nearly 5n nonzero entries in $L$, $n$ is the row number of matrices. The reduction in the residual is by a factor of $10^{15}$. }
%\label{anisotab}       % Give a unique label
%% For LaTeX tables use
%\begin{tabular}{cccccccc}
%\hline\noalign{\smallskip}
%mesh size& 300 & 400 &500&600&700&800&900  \\
%\noalign{\smallskip}\hline\noalign{\smallskip}
%iterations&47 & 51 & 56& 51 &53&53&54 \\
%%number & number & number \\
%\noalign{\smallskip}\hline
%\end{tabular}
%\end{table}
%
%%We remark that, when we test the Vaidya's augmented maximum spanning tree preconditioners by using the software of TAUCS, the execution go wrong when mesh size is greater than 500 in case of anisotropic
%%problem with strong influences in x direction or when mesh size is greater than 300 in case of anisotropic problem with strong influences in x direction on the distributed memory cluster. We don't know why. The result about the vaidya's preconditioners are tested on a 32 bit computer (3.10 GHz Intel Core i5 with 3 GB of main memory running windows ). Our preconditioners are OK on both platforms.
%
%%\begin{figure}[ht]
%%% Use the relevant command to insert your figure file.
%%% For example, with the graphicx package use
%% \includegraphics[height=5cm,width=0.9\textwidth]{aniso_iter.eps}
%%% figure caption is below the figure
%%\caption{The convergence of MST(augmented maximum weight spanning tree preconditioner), LST(our subgraph preconditioner based on low stretch spanning tree), IC(drop-tolerance incomplete Cholesky factorization) for anisotropic problems with Neumann  boundary conditions. The figure show iterations of PCG when converge with all the preconditioners having nearly $3.5n$ nonzeros in L. }
%%\label{anisoiter}       % Give a unique label
%%\end{figure}
%
%
%
%
%
%
%%\begin{figure}[ht]
%%% Use the relevant command to insert your figure file.
%%% For example, with the graphicx package use
%% \includegraphics[height=5cm]{lst_mst.eps}
%%% figure caption is below the figure
%%\caption{The convergence of MST(augmented maximum weight spanning tree preconditioner), LST(our subgraph preconditioner based on low stretch spanning tree) for anisotropic problems with Neumann boundary conditions. The figure show iterations of PCG when converge with all the preconditioners having nearly $3.5n$ nonzeros in L. }
%%\label{lm}       % Give a unique label
%%\end{figure}
%
% %As the experiments above show, our preconditioners have similar performance to Vaidya's preconditioners,which are both much better than IC preconditioners. To the end,  we test the time cost of the former two kinds of support graph preconditioners as the size of linear systems grows to compare the time complexity of the two.
%% %Figure \ref{iso_aniso_time} show that, our preconditioner have lower time cost than Vaidya's preconditioners for our tested examples.
%%%\begin{figure}[ht]
%%%% Use the relevant command to insert your figure file.
%%%% For example, with the graphicx package use
%%% \includegraphics[height=7cm]{iso_aniso_time.eps}
%%%% figure caption is below the figure
%%%\caption{The time cost of PCG when preconditioned by MST(augmented maximum weight spanning tree preconditioner), LST(our subgraph preconditioner based on low stretch spanning tree), for isotropic problems with Neumann and  for anisotropic problems with strong influences in y direction when matrix size grows. The figure show factor time + iterative time cost of PCG when converge. Preconditioners for isotropic problems have nearly $4n$ nonzeros in L, preconditioners for anisotropic problems have nearly $3.5n$ nonzeros in L.}
%%%\label{iso_aniso_time}       % Give a unique label
%%%\end{figure}
%%
%%
%%%\begin{table}[H]\fontsize{8.5pt}{12pt}\selectfont
%%%  \begin{center}
%%%  \caption{}\vspace{1pt}
%%%    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}*{9}{|c|c| c| c| c|c| c| c| c}} \cline{1-9}
%%%    \multirow{2}{*}{Matrix} & \multicolumn{4}{c|}{IC} & \multicolumn{4}{c|}{LST}\\
%%%    \cline{2-9}
%%%
%%%        & $t\_f$  & $t\_s$ & $it$ & $order$& $t\_f$  & $t\_s$ & $it$ & $order$\\   \cline{1-9}
%%%       data  & 2851  &33037 &11.5879& 0.0037 &4.3882e-006 &4.9612e+003 &binary&\\  \cline{1-9}
%%%
%%%
%%%  \end{tabular*}\label{tab:1}%\vspace{-15pt}
%%%  \end{center}
%%%\end{table}
%%
%%\begin{figure}[ht]
%%% Use the relevant command to insert your figure file.
%%% For example, with the graphicx package use
%% \includegraphics[width=1.0\textwidth]{akpw_eig.eps}%height=9.cm,
%%% figure caption is below the figure
%%\caption{The generalized eigenvalue of preconditioned systems for anisotropic problems with x-direction of strong influences and for isotropic problems with Dirichlet boundary condition.LST denote subgraph preconditioners obtained by Precon$\_$Create using tree-decomposition subroutine, MST denote Vaidya's augmented maximum spanning tree preconditioner, IC denote preconditioners of drop-tolerance incomplete Cholesky factorization.}
%%\label{iso_anisoeig}       % Give a unique label
%%\end{figure}
%%
%
%
%%
%%\begin{table}[H]\fontsize{8.5pt}{12pt}\selectfont
%%  \begin{center}
%%  \caption{}\vspace{1pt}
%%    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}*{11}{|c|c c c|c  c c|c}}\cline{1-8}    %|c|c c c|c  c c|c|                               \hline
%%     Matrix     &n &m &m/n(Sparsity)  &$\lambda_{max}$&  $\lambda_{min}$&$ \frac{\lambda_{max}}{\lambda_{min}}$& Type\\\cline{1-8}
%%    airfoil1  &4253   & 28831 &6.7790 &10.583 &1.2481e-004 &8.4792e+004 &binary\\
%%    vt2010  & 32580  & 188178 &5.7759 &58.063 &1.8751e-005 &3.0964e+006 &binary\\
%%    shock-9& 36476  & 179056 &4.9089 &7.9970 &7.3229e-006 &1.0921e+006 &binary\\
%%
%%    co2010  & 201062  & 1175636 &5.8471&121.04&2.4050e-006 &5.0329e+007 &binary\\
%%
%%        wv2010  & 135218  &798140 &5.9026& 95.025 &4.3882e-006 &2.1655e+007 &binary\\
%%        wy2010  & 86204  & 513790 &5.9602& 137.03 &7.5044e-006 &1.8260e+007 &binary\\
%%
%%        sd2010  & 88360  &499082&5.6483& 80.047 &6.3937e-006 &1.2520e+007 &binary\\
%%        sc2010  & 181908 &1075068 & 5.9100& 70.044 &2.9333e-006 &2.3879e+007 &binary\\
%%        stufe-10 & 24010& 72028 &2.9999 &6.5355 &1.6307e-006 & 4.0077e+006&binary    \\
%%    $commanche\_dual$& 7920 & 31680  &4.0000 & 5.9387 &4.6540e-005 & 1.2761e+005&binary \\
%%     biplane  & 21701&105777  &4.8743 &7.9960 &1.6859e-005 & 4.7428e+005&binary\\
%%     $delaunay\_n14$  & 16384  &114628 &6.9963& 17.388 &3.8848e-005& 4.4759e+005 &binary\\
%%
%%     nopoly  & 10774  & 70842 &6.5753 &11.622 &2.9877e-005&3.8899e+005 &binary\\
%%     crack  & 10240 &71000  &6.9336 &11.933 &4.3456e-005 &2.7460e+005 &binary \\
%%        data  & 2851  &33037 &11.588& 0.0037 &4.3882e-006 &4.9612e+003 &binary\\
%%    \cline{1-8}
%% \end{tabular*}\label{tab:1}%\vspace{-15pt}
%%  \end{center}
%%\end{table}
%
%

\subsection{Laplacian systems on Network graphs}
Another part of tested matrices (shown in Table \ref{tab_net}) are Laplacian matrices of unweighted graphs from the UF Sparse Matrix Collection \cite{UF}.
Next, we test the Laplacian systems arising from network graphs, including both mesh-like graphs and unstructured graphs. % All the tested matrices are list as follows (Table \ref{tab_net}).
We compare our preconditioners (ALST) to IC and Vaidya's preconditioners. %The results are listed in Table \ref{tab_net_time}.

Firstly, comparing the iterations and solution time, our ALST preconditioners outperform IC preconditioners and roughly better than Vaidya's preconditioners as shown in Table \ref{tab_net_time}.
Then, observing the stretch of low stretch spanning trees used for constructing ALST preconditioners and  maximum spanning tree used for constructing Vaidya's preconditioners, we can find low stretch spanning trees indeed have lower stretch than a random spanning tree. In fact, when graphs are unweighted, any random spanning tree is just a maximum spanning tree.  Besides, from the experimental results shown in Table \ref{tab_net_time} we find that the more sparser a matrix is, the better ALST preconditioners performs than Vaidya's preconditioners. Moreover, for large  matrices, ALST preconditioners are obviously more efficient  than Vaidya's preconditioners. So, the efficiency of our subgraphs preconditioners depends on not only the stretch  but also depends on the sparsity.
%condition number of matrices but also depends on the sparsity and matrix size.

  %the larger of a matrix is, the better AKPW preconditioners perform than Vaidya's preconditioners.

  % We guess  maybe the efficiency of subgraphs preconditioners depends on the sparsity and matrix size.


%We have cited the results of Boman et al. \cite{BKJ} and Hoske et al. \cite{DDH}  before(in the Introduction) that,
%while DRK did scale with near linear time, the performance of DRK is no better than PCG as the stretch of spanning trees is too large for DRK to be useful in practice. Although for small graph a   In our experiments, we use PCG even the condition number of the matrix is very large.





%Later, Boman et al. \cite{BKJ} evaluated the performance of DRK on a variety of real world graphs and proposed a parallel model of the Kelner et al. method.  However, these results do not support the practical utility of DRK. For mesh like graphs, PCG usually takes less work than DRK.




\begin{table}[!htbp]\fontsize{6pt}{12pt}\selectfont
  \begin{center}
  \caption{Tested matrices who are Laplacian matrices of unweighted network graphs from the UF Sparse Matrix Collection. Here $n$ is the number matrix rows, $m$ is the number of nonzeros in a matrix, and the sparsity $(m/n)$ is the average number of nonzeros in a column of a matrix. $\lambda_{min}$,  $\lambda_{max}$ are the minimal and maximal  eigenvalue of a matrix respectively, cond($A$) denotes the condition number of matrix $A$.}  \vspace{1pt}
    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}*{11}{|c|c c c|c  c c}}\cline{1-7}    %|c|c c c|c  c c|c|                               \hline
     Matrix     &$n$ &$m$ &sparsity($ m/n$)  &$\lambda_{max}$&  $\lambda_{min}$&$ \frac{\lambda_{max}}{\lambda_{min}}$\\\cline{1-7}%& Type
    airfoil1  &4253   & 28831 &6.7790 &10.583 &1.2481e-004 &8.4792e+004\\%&binary
    vt2010  & 32580  & 188178 &5.7759 &58.063 &1.8751e-005 &3.0964e+006 \\%&binary
    shock-9& 36476  & 179056 &4.9089 &7.9970 &7.3229e-006 &1.0921e+006 \\%&binary

    co2010  & 201062  & 1175636 &5.8471&121.04&2.4050e-006 &5.0329e+007 \\%&binary

        wv2010  & 135218  &798140 &5.9026& 95.025 &4.3882e-006 &2.1655e+007 \\%&binary
        wy2010  & 86204  & 513790 &5.9602& 137.03 &7.5044e-006 &1.8260e+007 \\%&binary

        sd2010  & 88360  &499082&5.6483& 80.047 &6.3937e-006 &1.2520e+007 \\%&binary
        sc2010  & 181908 &1075068 & 5.9100& 70.044 &2.9333e-006 &2.3879e+007 \\%&binary



        stufe-10 & 24010& 72028 &2.9999 &6.5355 &1.6307e-006 & 4.0077e+006    \\%&binary
    $commanche\_dual$& 7920 & 31680  &4.0000 & 5.9387 &4.6540e-005 & 1.2761e+005 \\%&binary
     biplane  & 21701&105777  &4.8743 &7.9960 &1.6859e-005 & 4.7428e+005\\%&binary
     $delaunay\_n14$  & 16384  &114628 &6.9963& 17.388 &3.8848e-005& 4.4759e+005\\%&binary

     nopoly  & 10774  & 70842 &6.5753 &11.622 &2.9877e-005&3.8899e+005\\%&binary
     crack  & 10240 &71000  &6.9336 &11.933 &4.3456e-005 &2.7460e+005 \\%&binary
        data  & 2851  &33037 &11.588& 0.0037 &4.3882e-006 &4.9612e+003\\%&binary
    \cline{1-7}
 \end{tabular*}\label{tab_net}%\vspace{-15pt}
  \end{center}
\end{table}


\begin{table}[!htbp]\fontsize{6.5pt}{12pt}\selectfont
  \begin{center}
  \caption{The solution time and iteration numbers of PCG take, under the constraint of all the preconditioners having nearly $4n$ nonzeros in the factor of preconditioners.  $t_s:$ solution time, $\#it:$ iteration numbers, $ave\_s:$ the average stretch of the spanning trees   built for constructing preconditioners.}\vspace{1pt}
    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}*{9}{|c|c c c| c c  c| c c c}} \cline{1-10}
    \multirow{2}{*}{Matrix} & \multicolumn{3}{c|}{IC} & \multicolumn{3}{c|}{ALST}& \multicolumn{3}{c|}{Vaidya} \\
    \cline{2-10}

        & $t\_s$ & $\#it$ & $ave\_s$& $t\_s$ & $\#it$ & $ave\_s$ &$t\_s$ & $\#it$ & $ave\_s$\\   \cline{1-10}
       airfoil1  & 0.078  &141 &-  & 0.039 &77 &7.04   & 0.053&87&8.90\\  \cline{1-10}
       vt2010 &0.437   &212 &-     & 0.174 &64 &6.08   & 0.203&68&8.34\\\cline{1-10}
       shock-9& 0.721  &309 &-  &0.279  &133 &11.9  & 0.484&158&19.4\\\cline{1-10}


      co2100    & 6.237  &459 &-   & 1.316 &96 &7.10  &2.337&106&10.1\\\cline{1-10}
        wv2010   & 4.258  &474 &-    &0.824  &93 &7.46  &1.422&102&9.89\\\cline{1-10}
        wy2010   & 1.729  &312 &-  & 0.411 &74 &6.35  &0.73&83&8.35\\\cline{1-10}
         sd2010   & 2.207  &395 &-   & 0.533 &95 &8.30  &0.88&100&9.34\\\cline{1-10}
         sc2010   & 6.222  &459 &- & 1.141 &93 &7.75 &2.194&109&9.18\\\cline{1-10}


         stufe-10  & 0.576 &254 &- &0.154  &113 &10.7   &0.246&125&12.3\\\cline{1-10}
         $commanche\_dual$ &0.121  &142 &-  & 0.049 & 59 &8.25   &0.061&67&8.82\\\cline{1-10}
         biplane    & 0.535  &246 &-    & 0.187 &107 &9.72   &0.205&118&16.8\\\cline{1-10}
          $delaunay\_n14$    & 0.399  &176 &- & 0.145 & 88&10.1   &0.156&95&12.2\\\cline{1-10}

            nopoly & 0.448  &314 &-      & 0.104 &75 &6.32  &0.072&77&21.3\\\cline{1-10}
          crack& 0.191  &161 &-       & 0.081 &76 &8.61  & 0.087&77&9.40\\\cline{1-10}
        data & 0.051 &104 &- &0.036  &77 &6.11    &0.033&74&6.73\\
       \cline{1-10}

  \end{tabular*}\label{tab_net_time}%\vspace{-15pt}
  \end{center}
\end{table}











\end{document}
% end of file template.tex

