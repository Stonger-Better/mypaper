%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
\usepackage{amssymb,amsmath}
\usepackage{array,tabularx,multirow}
\usepackage[linesnumbered,boxed]{algorithm2e}
%\usepackage{algorithm}
%\usepackage{algorithmic}
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}

%\newtheorem{lemma}{\bf{Lemma}}[section]

%\newtheorem{theorem}{\bf{Theorem}}[section]

%\newtheorem{definition}{Definition}[section]
% Insert the name of "your journal" with
% \journalname{myjournal}

\begin{document}

\title{A Preconditioning Algorithm for Large Linear Systems Based on A Low-stretch Spanning Tree% Based on a Low-stretch Spanning Tree  %\thanks{Grants or other notes
%about the article that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}
%A practical preconditioning algorithm for Laplacian(SDD) linear system
}
%\subtitle{Do you have a subtitle?\\ If so, write it here}

%\titlerunning{Short form of title}        % if too long for running head

\author{Huirong Zhang \and  Jianwen Cao \and Xiaohui Liu  %etc.
}%\inst{1,2}

%\authorrunning{Short form of author list} % if too long for running head

\institute{H.R. Zhang \at
              State Key Laboratory of Computer Science, Institute of Software Chinese Academy of Sciences. \\
              \email{zhang06.happy@163.com}           %  \\
%%             \emph{Present address:} of F. Author  %  if needed
           \and
           J.W. Cao \at
             State Key Laboratory of Computer Science, Institute of Software Chinese Academy of Sciences.\\
           \and
           X.H. Liu  \at
              State Key Laboratory of Computer Science, Institute of Software Chinese Academy of Sciences.\\
}

%\institute{  State Key Laboratory of Computer Science, Institute of Software Chinese Academy of Sciences. \\
%              \email{zhang06.happy@163.com}           %  \\
%%             \emph{Present address:} of F. Author  %  if needed
%           \and
%             State Key Laboratory of Computer Science, Institute of Software Chinese Academy of Sciences.\\
%          }


\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle

\begin{abstract}
In this paper, we present an efficient algorithm for preconditioning sparse, symmetric, diagonally-dominant(SDD) linear systems by using combinatorial preconditioning techniques.  Firstly, we build a low-stretch spanning tree of a graph associated with a linear system by using algorithm proposed by Alon et al. and  add appropriate high stretch edges to the tree straightly or add edges based on a tree-decomposition algorithm to get an optimized subgraph. Then, convert the subgraph into a SDD matrix and take it as a preconditioner. %solve a  SDD system $Ax=b$ preconditioned by a matrix converted from the subgraph we create.
Finally, we give an implementation and  performance analysis of our subgraph preconditioners. %We show experimentally that these combinatorial preconditioners  have robust convergence and  have good scalability.
We test the algorithm on extensive numerical experiments arising from both elliptic PDEs and  Laplacian systems of network graphs. Numerical experiments show that preconditioners constructed by our algorithm are more efficient than incomplete Choleskey factorization preconditioners and Vaidya's preconditioners. Moreover, our preconditioning algorithm is insensitive to the boundary condition and it scales well. %, which means our combinatorial preconditioning algorithm has good scalability.
%In addition, for the 2-D problems considered in this paper, preconditioners obtained based on a low stretch tree have better performance than  incomplete
%Cholesky preconditioners, which is sensitive to boundary  and anisotropy.
  Besides, the efficiency of our subgraphs preconditioners depends on not only the stretch  but also depends on the sparsity.  %But they are % and matrix size.


%\noindent {\bf AMS subject classifications: }
\keywords{preconditioning algorithm \and linear systems  \and low-stretch spanning tree \and tree-decomposition \and experimental analysis}
%\keywords{PCG Algorithm;  Augmented low stretch spanning tree; Tree-decomposition; SDD linear systems}
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}

%Moreover, the smallest generalized eigenvalues of a linear system preconditioned by a subgraph preconditioner is 1, just as analyzed in theory.
%The experiments on network graphs show that  when a spanning tree of the underlying graph of a matrix has  low  average stretch, %or small max stretch at the same time,
%a preconditioner constructed based on such a spanning tree applied with a CG solver converges fast.

\section{Introduction}\label{sec:0}
Conjugate gradient (CG) method \cite{ME52,Book1,Book2} is a widely used iterative method for solving
large sparse linear systems of the form $Ax=b$, where $A\in\mathbb{R}^{n\times n}$ is a symmetric, diagonally dominant (SDD) matrix. %with nonzero entries.
Such systems arise from the solution of certain elliptic differential equations, the modeling of resistive networks, and certain network optimization problems \cite{GG,LA,SF,UF}. The convergence rate of CG depends on the spectrum of $A$. When its eigenvalues are clustered or the condition number is small, the method converges rapidly. Most applied work on fast linear solvers has been on preconditioned conjugate gradient (PCG) solvers, including support graph preconditioners \cite{BHT,support,Germban}, or specialized multigrid methods \cite{KMT,LB}.

Solvers for linear systems in graph Laplacians have emerged as a powerful primitive for designing efficient algorithms. The ability to solve such systems in nearly-linear
time \cite{ST04} led to a variety of efficient algorithms eg. \cite{ST06,SW,SS08,KMP10,KMP11,Kern,DDH,BKJ}. Previously, following an
approach proposed by Vaidya \cite{vaidya}, %a nearly-linear time algorithm for solving linear systems in symmetric diagonally dominant matrices.
Spielman and Teng achieved a major breakthrough to propose a fast ST-solver to solve SDD liner system in nearly-linear time \cite{ST04,ST06,SW}. Later, the ST-solver was improved by Koutis et al. \cite{SS08,KMP10,KMP11} based on algorithms \cite{ABN,AKPW,EEST,petal}. But their algorithms do not yet have a practical implementation.
A new nearly-linear algorithm for solving linear systems was proposed by Kelner et al. in \cite{Kern}, denoted by DRK for short.
%A new algorithm for solving Laplace linear systems proposed by Kelner et al. \cite{Kern}, denoting by DRK algorithm for short.
Hoske et al. \cite{DDH} implemented the DRK algorithm and they concluded that while DRK did scale with near linear time, the stretch of spanning trees is too large to be useful in practice.
Later, Boman et al. \cite{BKJ} evaluated the performance of DRK on a variety of real world graphs and proposed a parallel model of the Kelner et al. method. However, these results do not support the practical utility of DRK. For mesh like graphs, PCG usually takes less work than DRK.

In this paper, we present a practical and efficient algorithm for preconditioning SDD linear system. And we give an implementation and performance analysis of the augmented low stretch spanning tree preconditioners. We give two schemes for adding edges to the tree to construct a support subgraph. One scheme is to add edges with highest stretch to the tree straightly. Another scheme is to %%%%%%%%%%%%%%%%%%%%%%这是我添的
 take advantage of the tree-decomposition algorithm introduced in \cite{BHT,Chen_Tov}  to add off-tree edges which have highest stretch between the subtrees get from the tree decomposition. %or ??

 We test our algorithm on extensive numerical examples arising from elliptic PDEs problems (considering  both isotropic and anisotropic cases) and unweighted graphs (including both mesh like graphs and unstructured network graphs). Applied with a CG solver, preconditioners constructed by our algorithm are more efficient than incomplete Choleskey factorization and Vaidya's preconditioners for all numerical experiments. %The experiment results also show that our preconditioning algorithm is more efficient than drop-tolerance incomplete Cholesky factorization(IC) preconditioners and Vaidya's preconditioners.
%The numerical experiments show that our preconditioning algorithm is more efficient than drop-tolerance incomplete Cholesky factorization(IC) preconditioners and Vaidya's preconditioners not only for linear systems arising from Poisson problems(considering  both isotropic and anisotropic cases), but also for  linear systems arising from unweighted graphs(including both mesh like graphs and unstructured network graphs). %The right hand sides are generated randomly.
%combinatorial preconditioners obtained by our algorithm  have robust convergence.
%They are insensitive to the numerics of the problem. Their performance does not vary much when we change the boundary conditions of the problems, or when we change the direction of anisotropy in anisotropic problems.
The test examples on elliptic PDEs problems also show that our augmented low stretch spanning tree preconditioning algorithm is insensitive to the boundary condition and it scales well as the number of iterations grow slowly as mesh size grows. %, which means our combinatorial preconditioning algorithm has good scalability.
%In addition, for the 2-D problems considered in this paper, preconditioners obtained based on a low stretch tree have better performance than  incomplete
%Cholesky preconditioners, which is sensitive to boundary  and anisotropy.
 Moreover, the smallest generalized eigenvalues of a linear system preconditioned by a subgraph preconditioner is 1, just as analyzed in theory.
The experiments on network graphs show that  when a spanning tree of the underlying graph of a matrix has  low  average stretch, %or small max stretch at the same time,
a preconditioner constructed based on such a spanning tree applied with a CG solver converges fast. Besides, the efficiency of our subgraphs preconditioners depends on not only the stretch  but also depends on the sparsity.  %But they are % and matrix size.





%
%  more sparser a matrix is, the better ALST preconditioners perform than Vaidya's preconditioners. Moreover, for large  matrices, ALST preconditioners are obviously more efficient  than Vaidya's preconditioners. So, the efficiency of our subgraphs preconditioners depends on not only the condition number of matrices but also depends on the sparsity and matrix size.

 %Its main ingredients are a low stretch spanning tree constructed by Alon et al. \cite{AKPW} and a tree decomposition algorithm implemented in \cite{BHT,Chen_Tov}. ?????????????????????????????
 %In addition, our preconditioners show robust convergence behavior.??? %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%我想说，我们的工作和 ST.et al 的工作有什么联系？？？

%our work is an extension of Vaidya's techniques.




%Then, observing the stretch of low stretch spanning trees used for constructing ALST preconditioners and  maximum spanning tree used for constructing Vaidya's preconditioners, we can find low stretch spanning trees indeed have lower stretch than a random spanning tree. In fact, for all the graphs are unweighted, any random spanning tree is just a maximum spanning trees.  Besides, from the experimental results shown in Table \ref{tab_net_time} we find that the more sparser a matrix is, the better ALST preconditioners perform than Vaidya's preconditioners. Moreover, for large  matrices, ALST preconditioners are obviously more efficient  than Vaidya's preconditioners. So, the efficiency of our subgraphs preconditioners depends on not only the condition number of matrices but also depends on the sparsity and matrix size.











% We implement the  algorithm for building a low stretch spanning tree constructed by Alon,Karp, Peleg, and West \cite{AKPW}. Then compute stretch for all the edges and construct the stretch optimized subgraphs based on the tree-decomposition algorithm or adding appropriate high stretch edges straightly.

% We consider isotropic problems and anisotropic problems on regular 2-dimensional meshes.
%We compare the performance of our preconditioners based on a low stretch tree to that of the  Vaidya's preconditoners based on a maximum spanning tree and the drop-tolerance %incomplete-Cholesky preconditioners (IC) by the use of a conjugate gradients iterative solver.


 The paper is organized as follows. Section \ref{sec:1} gives some preliminaries to illustrate . Section \ref{sec:2} provides the theoretical analysis of subgraph preconditioning. Section \ref{sec:3} presents our preconditioning algorithm applied with conjugate gradient (CG) method. Section \ref{sec:4} gives the numerical experiments to show the high efficiency of our preconditioners. Some conclusions remarks are given in section \ref{sec:5} and we point out the directions for further research.





 %We give two schemes for adding edges to the tree to construct a support subgraph. One scheme is to add edges with highest stretch to the tree straightly. Another scheme is to
% take advantage of the tree-decomposition algorithm introduced in \cite{BHT} and to add off-tree edges which have highest stretch between the subtrees get from the decomposition.

%We implement the  algorithm for building a low stretch spanning tree constructed by Alon, Karp, Peleg, and West \cite{AKPW}. Then compute stretch for all the edges and construct the stretch optimized subgraphs based on the tree-decomposition algorithm or adding appropriate  high stretch edges straightly. Then we test our combinatorial preconditioners to preconditioning matrices arise from finite-differences discretizations of elliptic PDEs on regular meshes. We consider isotropic problems and anisotropic problems on regular 2-dimensional meshes.
%We compare the performance of our preconditioners based on a low stretch tree to that of the  Vaidya's preconditoners based on a maximum spanning tree and the drop-tolerance incomplete-Cholesky preconditioners (IC) by the use of a conjugate gradients iterative solver.



%\subsection{Our work}
%%The key contribution of our work is an extension of Vaidya's techniques for constructing and analyzing combinatorial preconditioners. In this paper, we propose a simple combinatorial preconditioning algorithm based on a low stretch tree. We give two schemes for adding edges to the tree to construct a support subgraph. One scheme is to add edges with highest stretch  to the tree straightly. Another scheme is to
%% take advantage of the tree-decomposition algorithm introduced in \cite{BHT} and to add off-tree edges which have highest stretch between the subtrees get from the decomposition. With both two schemes  one can get a stretch optimized subgraph according to the support theory. However, the numerical experiments show the latter scheme is much better than the first scheme.
%We implement the algorithm for building a low stretch spanning tree constructed by Alon,Karp, Peleg, and West \cite{AKPW}. Then compute stretch for all the edges and construct the stretch optimized subgraphs based on the tree-decomposition algorithm or adding appropriate  high stretch edges straightly. Then we test our combinatorial preconditioners to preconditioning matrices arise from finite-differences discretizations of elliptic PDEs on regular meshes. We consider isotropic problems and anisotropic problems on regular 2-dimensional meshes.
%We compare the performance of our preconditioners based on a low stretch tree to that of the  Vaidya's preconditoners based on a maximum spanning tree and the drop-tolerance incomplete-Cholesky preconditioners (IC) by the use of a conjugate gradients iterative solver. All  of the tested matrices are generated by the software TAUCS.
%Finally, we give an experimental analysis to describe the  performance of our combinatorial preconditioners. The experiments show that our combinatorial preconditioners  have robust convergence.
%They are insensitive to the numerics of the problem. Their performance does not vary much when we change the boundary conditions of the isotropic problems or when we change the direction of anisotropy in anisotropic problems.
%
%Moreover, our combinatorial preconditioning algorithm has good scalability.





%\subsection{Paper Organization}
%
%Section \ref{sec:2} provides the analysis of combinational preconditioning in theory.  Section \ref{sec:3} presents our combinatorial preconditioning algorithm. Section \ref{sec:4} gives the numerical experiments. Section \ref{sec:5} gives some conclusions and directions for further research.
%
%
%
%The paper is organized as follows.
%
%%, which is of
%%independent interest. The paper is organized as follows.



%Its main ingredient is a new incremental graph sparsification algorithm, which is of
%independent interest. The paper is organized as follows.


%In this paper, we combine Vaidya's idea with Spielman and Teng's idea to give a simple algorithm for preconditioning by applying  tree constructed by Alon, Karp, Peleg, and West \cite{AKPW}.
%Boman and Hendrickson \cite{BH01} observed that by applying  support theory to the tree constructed by Alon,
%Karp, Peleg, and West \cite{AKPW}
%for the $k-$server problem, one obtains a spanning tree preconditioner $B$ with $\kappa(B^{-1}A)= m \,e^{\sqrt{logn \,loglog n}} $. They thereby  obtained a solver for linear systems in time $O(m^{3/2+o(1)}log(\kappa(B^{-1}A)/\epsilon))$.


%In this paper, we combine Vaidya's idea with Spielman and Teng's idea to give a simple algorithm for preconditioning by applying  tree constructed by Alon, Karp, Peleg, and West \cite{AKPW}.


%Boman and Hendrickson \cite{BH01} observed that by applying support theory to the tree constructed by Alon,
%Karp, Peleg, and West \cite{AKPW} for the
%$k-$server problem, one obtains a spanning tree preconditioner $B$ with $\kappa(B^{-1}A)= m \,e^{\sqrt{logn \,loglog n}} $. They thereby  obtained a solver for linear systems in time $O(m^{3/2+o(1)}log(\kappa(B^{-1}A)/\epsilon))$.
%
%In this paper, we extend Vaidya's idea to give a simple algorithm for preconditioning by applying  tree constructed by Alon,
%Karp, Peleg, and West \cite{AKPW} for the
%
%
%
%
%without use of spectral sparsification and recursive preconditioning.
%Boman and Hendrickson \cite{BH01} observed that by applying support theory to the tree constructed by Alon,
%Karp, Peleg, and West \cite{AKPW} for the
%$k-$server problem, one obtains a spanning tree preconditioner $B$ with $\kappa(B^{-1}A)= m \,e^{\sqrt{logn \,loglog n}} $. They thereby  obtained a solver for linear systems in time $O(m^{3/2+o(1)}log(\kappa(B^{-1}A)/\epsilon))$.
%
%In this paper we present a simple and  practical preconditioning algorithm. Applied with a CG solver, preconditioners constructed by our algorithm are more efficient than incomplete Choleskey facorization and . Its main ingredient is a new incremental graph sparsification algorithm, which is of
%independent interest. The paper is organized as follows.



%The time complexity of
%the ST-solver is at least $O(m log^{15}n)$. The large exponent in the logarithm is indicative of the fact that the algorithm is quite complicated and lacks practicality.
%The running time of their
%solver is a large number of polylogarithmic factors away from the obvious linear time lower bound. In later work, building upon further work of Spielman and Srivastava \cite{SS08,KMP10,KMP11} there have been many fast solvers based on low-stretch spanning trees for SDD systems. These fast algorithms all use tools of spectral sparsification and recursive preconditioning. The design of a faster and simpler solver is a challenging open question.
%In this paper we present a simple and  practical preconditioning algorithm.  Applied with a CG solver, preconditioners constructed by our algorithm are more efficient than incomplete Choleskey facorization and . Its main ingredient is a new incremental graph sparsification algorithm, which is of
%independent interest. The paper is organized as follows.



%However, they just experiment on 3-D  grids of small sizes.  Our interest lies in efficiency for large mesh graphs.



% To improve the spectrum distribution of $A$, one often use the preconditioned CG method
%(PCG) in which a preconditioner $B$ is used to approximate $A$. The
%convergence of PCG is determined by the spectrum of  $B^{-1}A$. %If a representation of $B^{-1}$
%can be constructed quickly and applied quickly, and if $B^{-1}A$ has a clustered spectrum or the spectrum condition number $\kappa(B^{-1}A)$ is small,
%PCG is very effective.

%Spielman and Teng [24], following an
%approach proposed by Vaidya [27], achieved a major breakthrough in this direction by devising
%a nearly-linear time algorithm for solving linear systems in symmetric diagonally dominant matrices.


%Combinatorial preconditioning is a relatively new technique
%that has gained attention in recent years. It is a technique that relies on graph algorithms to construct effective preconditioners. The simplest applications of
%combinatorial preconditioning targets symmetric diagonally-dominant (SDD) matrices with non-positive off diagonals, a class of matrices that are isomorphic to
%weighted undirected graphs. The coefficient matrix A is viewed as its isomorphic graph $G_A$.  A specialized graph algorithm constructs another graph $G_B$
%such that the isomorphic matrix $B$ is a good preconditioner for $A$.
%Combinatorial preconditioners are also called support graph preconditioners.
%Support graph preconditioners were first proposed by Vaidya \cite{vaidya},
%for symmetric, diagonally dominant matrices with positive diagonals. We call such matrices PSDDD as they are positive semi-definite and diagonally dominant. Such systems arise from the solution of certain elliptic differential equations via the finite element method, the modeling of resistive networks, and in the solution of certain network optimization problems \cite{GG,SF,LA}.

%In his seminal work, Vaidya shows that when $B$ corresponds to a subgraph of
%the graph of $A$, one can bound $\kappa(B^{-1}A)$ by bounding
%the dilation and congestion of the best embedding of the
%graph of $A$ into the graph of $B$. By using preconditioners derived by adding a few edges to maximum spanning trees, Vaidya’ s algorithm can solve PSDDD linear systems in time $O(n^{1.75})$.
%For PSDDD matrices whose graphs
%are planar, the total solution time is only $O(n^{1.2})$, which was implemented by Chen et al.\cite{Chen_Tov}.

%Boman and Hendrickson \cite{BH01} observed that by applying  support theory to the tree constructed by Alon,
%Karp, Peleg, and West \cite{AKPW} for the
%$k-$server problem, one obtains a spanning tree preconditioner $B$ with $\kappa(B^{-1}A)= m \,e^{\sqrt{logn \,loglog n}} $. They thereby  obtained a solver for linear systems in PSDDD
%matrices in time $O(m^{3/2+o(1)}log(\kappa(B^{-1}A)/\epsilon))$.  Recent research, largely motivated by the Spielman and Teng solver \cite{ST04}(ST-solver), demonstrates the
%power of SDD solvers as an algorithmic primitive. The ST-solver is an iterative algorithm that produces a sequence of approximate solutions converging
%to the actual solution of the input system Ax = b.




%Later, algorithmic advances were largely motivated by the seminal work of Spielman and Teng who gave the first nearly-linear time solver for SDD systems \cite{ST04,EEST}.
%The ST-solver is an iterative algorithm that produces a sequence of approximate solutions converging
%to the actual solution of the input system Ax = b.

%The time complexity of
%the ST-solver is at least $O(m log^{15}n)$. The large exponent in the logarithm is indicative of the fact that the algorithm is quite complicated and lacks practicality.
%The running time of their
%solver is a large number of polylogarithmic factors away from the obvious linear time lower bound. In later work, building upon further work of Spielman and Srivastava \cite{SS08,KMP10,KMP11} there have been many fast solvers based on low-stretch spanning trees for SDD systems. These fast algorithms all use tools of  spectral sparsification and recursive preconditioning. The design of a faster and simpler solver is a challenging open question.
%In this paper we present a simple and  practical preconditioning algorithm.  Applied with a CG solver, preconditioners constructed by our algorithm are more efficient than incomplete Choleskey facorization and . Its main ingredient is a new incremental graph sparsification algorithm, which is of
%independent interest. The paper is organized as follows.


%The ST-solver is an iterative algorithm that produces a sequence of approximate solutions converging
%to the actual solution of the input system Ax = b. The performance of iterative methods is commonly
%measured in terms of the time required to reduce an appropriately defined approximation error by a
%constant factor. Even including recent improvements on some of its components, the time complexity of
%the ST-solver is at least O(m log
%15
%n). The large exponent in the logarithm is indicative of the fact that
%the algorithm is quite complicated and lacks practicality. The design of a faster and simpler solver is a
%challenging open question.
%In this paper we pre
\end{document}
% end of file template.tex

