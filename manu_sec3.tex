%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
\usepackage{amssymb,amsmath}
\usepackage{array,tabularx,multirow}
\usepackage[linesnumbered,boxed]{algorithm2e}
%\usepackage{algorithm}
%\usepackage{algorithmic}
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}

%\newtheorem{lemma}{\bf{Lemma}}[section]

%\newtheorem{theorem}{\bf{Theorem}}[section]

%\newtheorem{definition}{Definition}[section]
% Insert the name of "your journal" with
% \journalname{myjournal}

\begin{document}

\title{A Preconditioning Algorithm for Large Linear Systems Based on A Low-stretch Spanning Tree% Based on a Low-stretch Spanning Tree  %\thanks{Grants or other notes
%about the article that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}
%A practical preconditioning algorithm for Laplacian(SDD) linear system
}
%\subtitle{Do you have a subtitle?\\ If so, write it here}

%\titlerunning{Short form of title}        % if too long for running head

\author{Huirong Zhang \and  Jianwen Cao \and Xiaohui Liu  %etc.
}%\inst{1,2}

%\authorrunning{Short form of author list} % if too long for running head

\institute{H.R. Zhang \at
              State Key Laboratory of Computer Science, Institute of Software Chinese Academy of Sciences. \\
              \email{zhang06.happy@163.com}           %  \\
%%             \emph{Present address:} of F. Author  %  if needed
           \and
           J.W. Cao \at
             State Key Laboratory of Computer Science, Institute of Software Chinese Academy of Sciences.\\
           \and
           X.H. Liu  \at
              State Key Laboratory of Computer Science, Institute of Software Chinese Academy of Sciences.\\
}

%\institute{  State Key Laboratory of Computer Science, Institute of Software Chinese Academy of Sciences. \\
%              \email{zhang06.happy@163.com}           %  \\
%%             \emph{Present address:} of F. Author  %  if needed
%           \and
%             State Key Laboratory of Computer Science, Institute of Software Chinese Academy of Sciences.\\
%          }


\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle

\begin{abstract}
In this paper, we present an efficient algorithm for preconditioning sparse, symmetric, diagonally-dominant(SDD) linear systems by using combinatorial preconditioning techniques.  Firstly, we build a low-stretch spanning tree of a graph associated with a linear system by using algorithm proposed by Alon et al. and  add appropriate high stretch edges to the tree straightly or add edges based on a tree-decomposition algorithm to get an optimized subgraph. Then, convert the subgraph into a SDD matrix and take it as a preconditioner. %solve a  SDD system $Ax=b$ preconditioned by a matrix converted from the subgraph we create.
Finally, we give an implementation and  performance analysis of our subgraph preconditioners. %We show experimentally that these combinatorial preconditioners  have robust convergence and  have good scalability.
We test the algorithm on extensive numerical experiments arising from both elliptic PDEs and  Laplacian systems of network graphs. Numerical experiments show that preconditioners constructed by our algorithm are more efficient than incomplete Choleskey factorization preconditioners and Vaidya's preconditioners. Moreover, our preconditioning algorithm is insensitive to the boundary condition and it scales well. %, which means our combinatorial preconditioning algorithm has good scalability.
%In addition, for the 2-D problems considered in this paper, preconditioners obtained based on a low stretch tree have better performance than  incomplete
%Cholesky preconditioners, which is sensitive to boundary  and anisotropy.
  Besides, the efficiency of our subgraphs preconditioners depends on not only the stretch  but also depends on the sparsity.  %But they are % and matrix size.


%\noindent {\bf AMS subject classifications: }
\keywords{preconditioning algorithm \and linear systems  \and low-stretch spanning tree \and tree-decomposition \and experimental analysis}
%\keywords{PCG Algorithm;  Augmented low stretch spanning tree; Tree-decomposition; SDD linear systems}
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}


\section{Theoretical analysis of subgraph preconditioning} \label{sec:2}
Vaidya suggested preconditioning Laplacian matrices of graphs by the Laplacians
of subgraphs of those graphs \cite{vaidya}. The family of preconditioners that followed
have been referred to as subgraph or combinatorial preconditioners, and the tools
used to analyze them are known as “support theory” \cite{DA}.
\subsection{Support theory}
%In \cite{BH}, Boman and Hendrickson provided a set of tools called support theory, which  can be viewed as an algebraic generalization of  support graph preconditioning techniques.
Support theory is suitable for all symmetric positive semi-definite matrices. It provides a set of tools to bound the generalized eigenvalues and
condition number for a matrix pencil $(A, B)$, where $B$ as being a preconditioner for $A$. Next, we introduce some definitions and tools in support theory.


The central concept in support theory is the support number of a matrix pair
$(A, B)$, sometimes simply called the support.
Support numbers generalize the notion of the maximal eigenvalue of a
matrix pencil.
\begin{definition}
(support number)
The support number for a matrix pencil $(A,B)$ is
$$\sigma(A,B)=min\{t~| \tau B\succeq A, \hbox{ for all } \tau\geq t \},$$
where $\tau B\succeq A$ means $\tau B-A$ is positive semi-definite.
\end{definition}
If $B$ is symmetric positive definite and $A$ is symmetric, then the support
number is always finite, if there is no such $t$ we define the support number
$\sigma(A, B)$ to be $\infty$.





\begin{theorem}\cite{BH}\label{sigma}
  Let $A$ and $B$ be symmetric, let $B$ also be positive semi-definite. If $null(B)\subset null(A)$, then
  $$\sigma(A,B)=max\{\lambda| Ax = \lambda Bx,Bx\neq 0\}.$$
  Specially, when $A$ and $B$ are symmetric positive definite, then
$\kappa(A, B)=\sigma(A, B)\sigma(B,A)$.
\end{theorem}


\begin{theorem}\cite{BH}\label{product}
  (Symmetric Product Support). Suppose $U\in \mathbb{R}^{n\times k}$ is in the range of $V\in \mathbb{R}^{n\times p}. $ Then
  $$\sigma(UU^T,VV^T)=\min_W\| W \|_2^2 \hbox{ subject to } VW=U,$$
   equality is achieved for $W = V^{+}U$.
\end{theorem}

The proof of Theorems \ref{sigma} and \ref{product} are provided in \cite{BH}.

One special case of interest is the columns of $U$ are a subset of the columns
of $V$ (or vice versa).

\begin{corollary}\label{sub}
  Suppose the columns of $U$ are a subset of the columns of $V$, then $\sigma(UU^T,VV^T)\leq 1.$
\end{corollary}
As the columns of $U$ are a subset of the columns of $V$, take $W$ as a submatrix of identity matrix, $\| W \|_2^2\leq 1.$%???? %%%%%%%%%%%%%%%%%这个需要解释，干脆把这个删了吧？？？
\subsection{Spectral bounds on subgraph preconditioning}
Next, we will use the isomorphism relation between graphs and Laplacian matrices to give a combinational bound on subgraph  preconditioning. From \cite{factor} we know
symmetric $M$-matrices have a $width$-2 factorization. This means that if $A$ is
a symmetric $M$-matrix, then it can be expressed as $A = UU^T$ for some matrix
$U$ that has at most two non-zero entries  of opposite signs per column.

When $A$ and $B$ are diagonally dominant symmetric matrices, there is a simple way to factor $A$ and $B$ such that $U$ and $V$ are  as sparse as $A$ and $B$.
In \cite{BHT}, Boman et al. gave a factorization to represent $A$ as a sum of rank-1 matrices and to represent $B$ as $B=VV^T$. These representations rely on the
following definitions of edge vectors and vertex vectors.





\begin{definition}
  (edge vector and vertex vector)
  Let $1\leq i,j \leq n$, $i\neq j.$ The positive edge vector  $\langle i,-j\rangle$ and the negative edge vector $\langle i,j\rangle$ are defined by

$\langle i,-j\rangle_k=\left\{
                  \begin{array}{ll}
                    +1,~& \hbox{$k$=$i$} \\
                    -1, ~& \hbox{$k$=$j$} \\
                    0, ~& \hbox{$otherwise$,}
                  \end{array}
                \right.$
and \quad
$\langle i,j\rangle_k=\left\{
                  \begin{array}{ll}
                    +1,~& \hbox{$k$=$i$} \\
                    +1,~& \hbox{$k$=$j$} \\
                    0,~ & \hbox{$otherwise$.}
                  \end{array}
                \right.$

                A vertex vector $\langle i\rangle$ is the unit vector
$$\langle i\rangle_k=\left\{\begin{array}{ll}
               +1,~& \hbox{$k$=$i$}\\
               0, ~& \hbox{$otherwise$.}
             \end{array}
                \right.$$


\end{definition}


As we have referred symmetric diagonally dominant matrices can be reduced to Laplacian matrices, it suffices to consider Laplacian matrices.
 The next corollary obtained from Lemma 4.2 in \cite{BHT} shows how to represent a Laplacian matrix $A$ as a sum of rank-$1$ matrices $u_ku^T_k$ where each $u_k$  is an edge vector or a vertex vector  of $A$.

\begin{corollary}
(Incident factorization)
  If $A\in \mathbb{R}^{n\times n}$ is a Laplacian matrix with zero row-sum,  $A$ can be split into ：

\begin{equation}\label{incident}
   A= \sum_{A_{ij}\neq 0, i<j} A_{ij} \langle i,-j \rangle  \langle i,-j \rangle ^T,
\end{equation}
 we called such a form of $A$ as an incident factorization.
\end{corollary}





Define $U$ to be the matrix whose columns are $ \sqrt{A_{ij}} \langle i,-j \rangle $, $(i,j)\in E$,
then $A$ is  isomorphic to a undirected graph $G=(V,E,\omega)$ with non-negative edge weight $\omega(i,j)=A_{ij}$ and $A=UU^T$.

To bound $\sigma(A,B)$ using Theorem \ref{product}, we need to factor $A$ and $B$ into $A=UU^T$ and $B=VV^T$, and we need to find a $W$ such that $U=VW$.
 %We have seen that if $A$ and $B$ are diagonally dominant, then there
%is an almost trivial way to factor $A$ and $B$ such that $U$ and $V$ are about
%as sparse as $A$ and $B$. But how do we find a $W$ such that $U=VW$?
Next, we show that when $A$ and $B$ are Laplacian marices we can define $W$ by using path embedding. What's more, we prove that the bounds on $\| W \|_2^2$ can be viewed as combinatorial bounds on the quality of the embedding.

%we can equivalently  bound $\| W \|_2^2$.
%However, the difficulty is how to find a $W$ such that $U=VW$.

%When $A$ is a Laplacian matrix, denote $G$ be the underlying graph of $A$, $A$ has an incident factorization in the form of (\ref{incident}). Our goal is to find a good subgraph $H$ of $G$ and take the Laplacian matrix $B$ of $H$ as a preconditioner of $A$. The Then we can define $W$ by using path embeddings.

%???????????%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%此前后春风学姐，提示要做大的改善？？？
Firstly, we consider the subgraph $H$ as a spanning tree of $G$,  every edge $e$ in $G$ corresponds to  a unique path  in $H$ and all the edge vectors of $H$ are independent.


According to Theorem 9.5 in \cite{BH}, it's easy to obtain the following result.
\begin{theorem}\label{basic}
Given Laplacian matrices $A\in\mathbb{R}^{n\times n}$ with underlying graph $G=(V,E,\omega)$, $|E|=m$, and $B \in\mathbb{R}^{n\times n}$ with underlying graph $T=(V,F,\omega)$, suppose $T$ be a spanning tree of $G$, denote $Path$ be the unique mapping of the edges in the graph $G$ onto paths in $T$. Let $A=UU^T$, $B=VV^T$ are incident factorizations in form of (\ref{incident}), $U\in\mathbb{R}^{n\times m}$, $V\in\mathbb{R}^{n\times (n-1)}$.
For every edge $e\in E(G)$,  $Path(e)$ be the unique  path in $T$ connecting endpoints of $e$.
We can define a matrix $W\in \mathbb{R}^{(n-1)\times m}$ as follows,
$$W_{f,e}=(-1)^{j_1>j_2}\frac{\sqrt{A_{i_1,i_2}}}{\sqrt{B_{j_1,j_2}}},$$
where $e=(i_1,i_2)\in G$ and $f=(j_1,j_2)\in T$, if $f\in Path(e)$.
Then $U=VW$ and
\begin{equation}\label{sigma_stretch}
  \sigma(A,B)\leq stretch(E)=\sum_{\scriptstyle e\in E} \sum_{f=(j_1,j_2)\in Path(i_1,i_2)}\frac{A_{i_1,i_2}}{B_{j_1,j_2}}.
\end{equation}

Furthermore,

\begin{equation}\label{max_stretch}
  \sigma(A,B)\leq \max_{f\in T}\sum_{f\in Path(e),e\in G}stretch(e),
\end{equation}
which is a tighter  support bound than (\ref{sigma_stretch}).
% = \sum_{\scriptstyle (j_1,j_2)\in P(i_1,i_2)}\frac{A_{i_1,i_2}}{B_{j_1,j_2}}
\end{theorem}

\begin{proof}

 For $T$ is a spanning tree of $G$,  every edge $e=(i_1,i_2)\in G$ with $i_1<i_2$  corresponds to  a unique path $Path(e)=(k_1=i_1,k_2,\cdots,k_l=i_2)$  in $T$. It is straightforward  to verify that $$\langle i_1,-i_2 \rangle=\sum_{j=1}^{l-1}\langle k_j,-k_{j+1} \rangle.$$

   As $A=UU^T$, $B=VV^T$ are incident factorizations in form of (\ref{incident}), for every edge $e=(i_1,i_2)\in G$ with $i_1<i_2$, $e$ corresponds to a column $U_e=\sqrt{A_{i_1,i_2}} \langle i_1,-i_2 \rangle $, and for every edge $f=(j_1,j_2)\in T$ with $j_1<j_2$, $f$ corresponds to a column $V_f=\sqrt{B_{j_1,j_2}} \langle j_1,-j_2 \rangle $.

   Then, it holds that $$U_e=\sqrt{A_{i_1,i_2}} \langle i_1,-i_2 \rangle =\sum_{j=1}^{l-1}\sqrt{A_{k_j,k_{j+1}}}\langle k_j,-k_{j+1} \rangle,$$
   and
   $$\langle j_1,-j_2 \rangle=\frac{1}{\sqrt{B_{j_1,j_2}}}V_f.$$

    Every column $U_e$ in $U$ can be written as a  linear combination of the columns $V_f=\sqrt{B_{j_1,j_2}} \langle j_1,-j_2 \rangle $ of $T$ as follows,
  \begin{equation}
    U_e = \sum_{f=(j_1,j_2)\in Path(e)}\frac{\sqrt{A_{i_1,i_2}}}{\sqrt{B_{j_1,j_2}}}\langle j_1,-j_2 \rangle.
  \end{equation}
  where $(j_1,j_2)=(\min(k_j,k_{j+1}),\max(k_j,k_{j+1}))$.
  According to the definition of $W$, we have
  $$U_e=VW_{:,e},$$
  where $W_{:,e}$ is the column of $W$ corresponding to the edge $e\in G$, with entries $W_{f,e}=\frac{\sqrt{A_{i_1,i_2}}}{\sqrt{B_{j_1,j_2}}}\neq 0$ iff $f=(j_1,j_2)\in Path(e)$ and $e=(i_1,i_2)\in G$.
  Assemble all the columns of $U$, we get $U=VW$.

  Next, we prove   $$\sigma(A,B)\leq stretch(E)=\sum_{e=(i_1,i_2)\in E}stretch(e)=\sum_{\scriptstyle e\in E} \sum_{f=(j_1,j_2)\in Path(i_1,i_2)}\frac{A_{i_1,i_2}}{B_{j_1,j_2}}.$$

  According to the definition of stretch (\ref{stretch}) and definition of $W$, we have $$\| W\|_F^2=\sum_{(i_1,i_2)\in G}\sum_{(j_1,j_2)\in P(i_1,i_2)}\frac{A_{i_1,i_2}}{B_{j_1,j_2}}=\sum_{e=(i_1,i_2)\in E}stretch(e).$$
  According to Theorem \ref{product}, $\sigma(A,B)\leq \| W\|_2^2\leq \| W\|_F^2=stretch(E).$

   In \cite{CGT}, Chen et al. gave a  sparse bound on the 2-norm of $W$ $$ \| W\|_2^2\leq \max_{f\in T} \sum_{f:W_{f,e}\neq 0} \| W(:,e)\|_2^2.$$ As $\| W(:,e)\|_2^2=stretch(e)$ and $\sigma(A,B)\leq \| W\|_2^2$, inequality (\ref{max_stretch}) holds.
\end{proof}



Further, we can easily extend the results of Theorem \ref{basic} to the case of that $A$ is symmetric diagonally dominant.

\begin{theorem}\label{main}
 If $A\in \mathbb{R}^{n\times n}$ is a nonsingular symmetric diagonally dominant matrix with non-positive off diagonal entries  and  positive diagonals,  one can construct a  combinatorial preconditioner by following steps.
  \begin{itemize}
    \item split $A$ into $A=D+L$, where $L$ is a Laplacian with zero row sums and $D$ is a non-negative diagonal matrix;
    \item suppose $G=(V,E,\omega)$ be the underlying graph of $L$, find a subgraph subgraph $H=(V,F,\omega)$ of $G$ with a path embedding $P$, denote $\widetilde{L}$ be the Laplacian matrix of $H$ with zero row sums, the stretch of $E$ with respect to $H$ is $stretch(E)$;
    \item take $B=D+\widetilde{L}$ as a preconditioner of $A$.
  \end{itemize}

Preconditioners $B$ constructed in this way have following properties:


\begin{itemize}
  \item [\textbf{P1}]\begin{equation}\label{stbound}
  1\leq \lambda_i\leq stretch(E), \,i=1,\cdots,n,
\end{equation}
 where $\lambda_i,\,i=1,\cdots,n$ are the $i$-th generalized eigenvalues of $\lambda(A,B)$.
  \item [\textbf{P2}]Suppose $e=(i_1,i_2)\in E$ be an edge of $G$, denote the embedded path under embedding $P$ be $Path(e)=e_{j_1},e_{j_2},\cdots,e_{j_k}$, where $e_{j_i}\in F$ are edges of $H$, consider the sparsity of a matrix, there is tighter combinatorial
bounds
\begin{equation}\label{maxbound}
  1\leq \lambda_i\leq \max_{(j_1,j_2)\in H} \sum_{(i_1,i_2\in G) (j_1,j_2)\in P(i_1,i_2))}stretch(i_1,i_2),
\end{equation}
  \item [\textbf{P3}]Such a preconditioner $B$ of $A$ is also nonsingular and has a property that preserves row-sum, namely $\sum_j A_{ij}=\sum_jB_{ij},\,i=1,\cdots,n$.
\end{itemize}
\end{theorem}

\begin{proof}
  Firstly, we prove property P1.
  As $A$ is symmetric and $B,D$ are positive semi-definite, we can use  Lemma 5.5 in \cite{BH} to obtain $$\sigma(A,B)=\sigma(D+L,D+\widetilde{L})\leq \sigma(L,\widetilde{L}).$$ According to Theorem \ref{sigma} and Theorem \ref{basic}, we know $\lambda_i\leq\lambda_{max}\leq stretch(E) $ for all $i=1,\cdots,n$. As $A$ is nonsingular, it is irreducible or it has strictly diagonally dominant rows, which means $L$ is irreducible or $D$ has  positive entries. If $L$ is irreducible, which means $G$ is connected, $\widetilde{L}$ is also  irreducible as it's  connected too. If $D$ has  positive entries, according to $B=D+\widetilde{L}$, we know $B$ also has strictly diagonally dominant rows. In conclusion, $B$ is nonsingular. As Laplacian matrices have zero row-sum,  $\sum_j A_{ij}=D(i,i)=\sum_jB_{ij},\,i=1,\cdots,n$. the proof of Property P3 is completed.

  As $A,B$ are both nonsingular, from Corollary \ref{sub}, we get $\frac{1}{\lambda(A,B)}\leq\sigma(B,A)\leq 1$, namely, $\lambda(A,B)\geq 1$.
  Property P1 is proved.

  Property P2 is a straightforward result of the  support bound (\ref{max_stretch}) in Theorem \ref{basic}.
  %???????

\end{proof}


%\subsection{Subgraph preconditioning effect on iterative algorithm}
 Theorem \ref{main} tells us, if we can build a low stretch spanning tree, the corresponding subgraph preconditioner can preconditioning a linear system well.
 Properties P1 and P2 in Theorem \ref{main} inspire us, we can build a subgraph based on a low stretch tree by decreasing max stretch or total stretch. %From now on, there are some algorithms for building a low stretch spanning tree \cite{ABN,AKPW,EEST,petal}.


  %Alon et al. proposed an algorithm  \cite{AKPW}
  %the best  bound  on the total stretch is $O(mlogn\,loglogn)$.
  In next section, we give our algorithm for building such a subgraph.

%If one uses the preconditioned conjugate gradient (PCG) to solve a linear equation,
%the PCG converge in $O(\kappa_f(A,B))$ iterations. %only when the generalized eigenvalues are distributed  poorly between $\lambda_{min}(A,B)$ and $\lambda_{max}(A,B)$.
%The following theorem tell us

%From Theorem \ref{main}, we know if a spanning tree has low stretch, subgraph preconditioners preconditioned linear systems have good spectral distribution.
%
%
%In \cite{SW}, Spielman and Woo  summarize the following theorem by using the analysis of Axelsson and Lindskog \cite{AL86} to show the convergence of PCG from the point of spectral distribution.
%
%\begin{theorem}\label{pre_iter}
%    Let $A$ and $C$ be positive semi-definite matrices with the same nullspace such that all but $q$ of the eigenvalues of $C^{\dag}A$ lie in the interval $[l,u]$, and the remaining $q$ are larger than $u$. If $b$ is in the span of $A$ and one uses the Preconditioned Conjugate Gradient with $C$ as a preconditioner to solve the linear system $Ax=b$, then after
%  $$k=q+\lceil\frac{ln(2/\epsilon)}{2}\sqrt{\frac{u}{l}}\rceil$$
%  iterations, the algorithm will produce a solution $x$ satisfying
%  $$\| x-A^{\dag}b\|_A\leq\epsilon\| A^{\dag} \|_A,$$
%  where $\| x\|_A\overset{def}{=}\sqrt{x^TAx}.$
%\end{theorem}
%
%Theorem \ref{pre_iter} show that if one can find  preconditioners such that  the eigenvalues of preconditioned systems are clustered or preconditioned systems have few large eigenvalues, PCG can converge fast.
%%%%%%%%%%%%%%%%%%%%%%%%这一小节，春风学姐说很重要，需要表明动机？？？？, 改进的地方
% ????????

\end{document}
% end of file template.tex

